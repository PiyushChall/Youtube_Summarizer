{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiyushChall/Youtube_Summarizer/blob/main/Youtube_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-JGV81K_HkG"
      },
      "source": [
        "# **Semantic Chunking of a Youtube Video**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HC1Y06k-8f-"
      },
      "source": [
        "# Installing modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6ac8_c3J1Lx"
      },
      "source": [
        "Installing Faster-Whisper as it allows:\n",
        "1. GPU accelaration\n",
        "2. Ease of use\n",
        "3. Directly Calculates chuncks of the video/audio\n",
        "4. High accuracy and precision\n",
        "5. Supports various languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gDXDXzhq8yL",
        "outputId": "7834d8fb-f55b-48cb-ba71-24517ac969bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faster-whisper\n",
            "  Downloading faster_whisper-1.0.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting av<13,>=11.0 (from faster-whisper)\n",
            "  Downloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ctranslate2<5,>=4.0 (from faster-whisper)\n",
            "  Downloading ctranslate2-4.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (192.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (0.23.1)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper) (0.19.1)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n",
            "  Downloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (1.25.2)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.11.0)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.12)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
            "Installing collected packages: humanfriendly, ctranslate2, av, coloredlogs, onnxruntime, faster-whisper\n",
            "Successfully installed av-12.0.0 coloredlogs-15.0.1 ctranslate2-4.3.0 faster-whisper-1.0.2 humanfriendly-10.0 onnxruntime-1.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install faster-whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgdKzWQyKtZh"
      },
      "source": [
        "Installing Pytube as it allows to downloads high resolution youtube Video directly through python code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAit7dMKDJpk",
        "outputId": "8a2ee392-6cd3-470d-fb1a-d5a59e9b710d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKsS4IlNLWeV"
      },
      "source": [
        "Using Gemini AI for summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KIGlldAlA2fU"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMKoFVXY_ELJ"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v-UezqGIDJgW"
      },
      "outputs": [],
      "source": [
        "from pytube import YouTube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6MxANklEtTq1"
      },
      "outputs": [],
      "source": [
        "from faster_whisper import WhisperModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MQk-zXrmu1xj"
      },
      "outputs": [],
      "source": [
        "from moviepy.editor import VideoFileClip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Bwio2BzBP1GE"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZeeBXTs8Gn5E"
      },
      "outputs": [],
      "source": [
        "from os import stat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "urYAY9RXAy00"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZEy-0T0fItto"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MEk7dj-sBJcn"
      },
      "outputs": [],
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-ma-Q8E_BPFH"
      },
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kubbqh9gFDzI"
      },
      "source": [
        "# Selecting Device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppIrzQehLPEf"
      },
      "source": [
        "This block of code directly identifies the connected device and assigns it to a variable for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xV__tD6JPylC"
      },
      "outputs": [],
      "source": [
        "processing_device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTxKdCcBLeTR"
      },
      "source": [
        "This block of code automatically assigns compute_type according to the connected device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fGmhp1sMP4k0"
      },
      "outputs": [],
      "source": [
        "if processing_device == \"cpu\":\n",
        "  computing_type = \"float32\"\n",
        "else:\n",
        "  computing_type = \"float16\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCmUPo-PHdF8"
      },
      "source": [
        "# Getting Video-Link from the user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "h7ICLMnAFdcn"
      },
      "outputs": [],
      "source": [
        "def user_input():\n",
        "  user_link = input(\"Enter the YouTube link: \")\n",
        "  user_format = input(\"Enter download format (highest or progressive) [highest]: \") or \"highest\"\n",
        "  return user_link, user_format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WquGBBrGNcsl"
      },
      "source": [
        "# Video Downloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xicGSot_LjFe"
      },
      "source": [
        "Function for downloading youtube video through our python code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IomMc9wiEkT0"
      },
      "outputs": [],
      "source": [
        "def download_video(link, resolution=\"highest\"):\n",
        "    try:\n",
        "        youtube_object = YouTube(link)\n",
        "        if resolution == \"highest\":\n",
        "            video = youtube_object.streams.get_highest_resolution()\n",
        "        else:\n",
        "            video = youtube_object.streams.filter(progressive=True).order_by('resolution').desc().first()  # Filter progressive downloads by descending resolution\n",
        "        return video.download()\n",
        "        print(f\"Downloaded: {youtube_object.title}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Something went wrong: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKiw-FlNNnaE"
      },
      "source": [
        "# Audio Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05lZoWtKMA6H"
      },
      "source": [
        "Function for extracting audio through our downloaded video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NzmKz_lHu4px"
      },
      "outputs": [],
      "source": [
        "def video2audio(v_path):\n",
        "  video_clip = VideoFileClip(v_path)\n",
        "  audio_clip = video_clip.audio\n",
        "\n",
        "  # Save the extracted audio\n",
        "  audio_clip.write_audiofile(\"extracted_audio.wav\")\n",
        "  audio_input = \"extracted_audio.wav\"\n",
        "  return audio_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xS2k7_0B-mK"
      },
      "source": [
        "# Model selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfgCJFIAAXlO"
      },
      "source": [
        "**Use this code accordingly in the below code** if you have Gpu for faster processing or cpu for slower processing!!\n",
        "\n",
        "(**FASTER**) [Suggested] For **GPU**:  \n",
        "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
        "\n",
        "(**SLOWER**) For **CPU**:  \n",
        "model = WhisperModel(model_size, device=\"cpu\", compute_type=\"float32\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "j5SSPbfIxRgR"
      },
      "outputs": [],
      "source": [
        "def model_select(processing_device, computing_type):\n",
        "  model_size = \"large-v3\"\n",
        "  model = WhisperModel(model_size, device=processing_device, compute_type=computing_type)\n",
        "  gemini_model = genai.GenerativeModel('gemini-pro')\n",
        "  return model, gemini_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HYWmzj9N_Ue"
      },
      "source": [
        "# Transcribing Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVCRX_iAMgBv"
      },
      "source": [
        "Lists to hold all the infos genrated from the transcribtion of the audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QJnjBNt6WpNn"
      },
      "outputs": [],
      "source": [
        "start_time = []\n",
        "end_time = []\n",
        "chunk_length = []\n",
        "text = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kpNRmxvMXhw"
      },
      "source": [
        "Function for extracting text from our extracted audio file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "R1V8-gS0uUw-"
      },
      "outputs": [],
      "source": [
        "def transcribe(a_input, model):\n",
        "  segments, info = model.transcribe(a_input, beam_size=10)\n",
        "  print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
        "  for segment in segments:\n",
        "    start_time.append(segment.start)\n",
        "    end_time.append(segment.end)\n",
        "    chunk_length.append(segment.end - segment.start)\n",
        "    text.append(segment.text)\n",
        "    paragraph = \" \".join(text)\n",
        "\n",
        "      # Add proper paragraph breaks\n",
        "    paragraph = paragraph.replace(\". \", \".\\n\")\n",
        "    paragraph = paragraph.replace(\"! \", \"! \\n\")\n",
        "    paragraph = paragraph.replace(\"? \", \"? \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jgk1pf9KBSif"
      },
      "outputs": [],
      "source": [
        "def summarize_and_extract_key_info(gemini_model, paragraph, max_tokens=100):\n",
        "  \"\"\"\n",
        "  Summarizes a chunk of text and extracts key information using Gemini AI Api.\n",
        "\n",
        "  Args:\n",
        "      text_chunk: String containing the chunk of text to be processed.\n",
        "      max_tokens: Maximum number of tokens allowed in the summary (default: 100).\n",
        "\n",
        "  Returns:\n",
        "      A dictionary containing the summary and key information.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Prompt for conversation summarization\n",
        "  prompt = (f\"Summarize the following paragraph, Try to make it easy to understand:\\n{paragraph}\")\n",
        "\n",
        "  # Send request to Gemini API\n",
        "  response = gemini_model.generate_content(prompt)\n",
        "\n",
        "  # Return the conversation summary\n",
        "  return response.text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fycXtZanHrXy"
      },
      "source": [
        "# Printing all values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwUAlEQ0M7Mp"
      },
      "source": [
        "This function allows to print all the data in a proper manner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Fxy9uG4llDfr"
      },
      "outputs": [],
      "source": [
        "def output(text, chunk_length, end_time, start_time, gemini_model):\n",
        "  for a,b,c,d in zip(text, chunk_length, end_time, start_time):\n",
        "    print(f\"Chunk Start Time: {d}seconds, Chunk End Time: {c}seconds, Chunk Length: {b}seconds,\")\n",
        "    print(f\"Chunk text: \\033[1m{a}\\033[0m\") # \"\\033[1m{a}\\033[0m\" will make the text bold in the terminal and easier to read\n",
        "  conversation_summary = summarize_and_extract_key_info(gemini_model, text)\n",
        "  print(f\"Summary: \\033[1m{conversation_summary}\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rPd5PrNKA_rn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSdUD8e5PFBd"
      },
      "source": [
        "# Main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QNyHfQGyNIgh"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  user_link, user_format = user_input()\n",
        "  video_path = download_video(user_link, user_format.lower())\n",
        "  if video_path != None:\n",
        "    audio_input = video2audio(video_path)\n",
        "  else:\n",
        "    print(\"Something went wrong!\")\n",
        "  model, gemini_model = model_select(processing_device, computing_type)\n",
        "  transcribe(audio_input, model)\n",
        "  output(text, chunk_length, end_time, start_time, gemini_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMvm91wSOyyt"
      },
      "outputs": [],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qsQFCZ0UXaZD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNAKxOHyosJsXy5LBDUW5h0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}